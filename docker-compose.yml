version: '3.8'

services:
  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: abby-ollama
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Abby Unleashed Core application
  abby:
    build: .
    container_name: abby-core
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - STT_MODEL=${STT_MODEL:-base.en}
      - TTS_VOICE=${TTS_VOICE:-en_US-amy-medium}
      - WAKE_WORD=${WAKE_WORD:-hey abby}
    volumes:
      # Persist memory and persona library
      - ./memory:/app/memory
      - ./persona_library:/app/persona_library
      - ./config:/app/config
      - ./logs:/app/logs
    ports:
      - "8080:8080"
    restart: unless-stopped
    command: ["python", "api_server.py", "--host", "0.0.0.0", "--port", "8080"]
    stdin_open: true
    tty: true
    # For voice mode, need host audio device access
    # devices:
    #   - /dev/snd:/dev/snd
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  ollama-data:
    driver: local

networks:
  default:
    name: abby-network

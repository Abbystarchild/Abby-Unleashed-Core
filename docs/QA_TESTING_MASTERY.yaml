# =============================================================================
# QA / TESTING ENGINEERING MASTERY - Professional Best Practices
# =============================================================================
# Comprehensive guide covering Test Pyramid, Automation, E2E Testing,
# Performance Testing, API Testing, and Test Data Management
# Version: 1.0.0
# Last Updated: 2026-02-02
# =============================================================================

metadata:
  domain: "QA / Testing Engineering"
  level: "Mastery"
  focus: "Verifiable, Production-Grade Practices"
  technologies:
    - Pytest
    - Playwright
    - Cypress
    - k6
    - Postman/Newman
    - Factory Boy
    - Pact (Contract Testing)
    - Locust

# =============================================================================
# SECTION 1: TEST PYRAMID
# =============================================================================
test_pyramid:
  overview: |
    The test pyramid is a strategic framework for balancing test coverage,
    execution speed, and maintenance cost. Proper ratios ensure fast feedback
    loops while maintaining confidence in system behavior.

  pyramid_ratios:
    description: "Recommended distribution of test types"
    ratios:
      unit_tests:
        percentage: "70%"
        characteristics:
          - "Fastest execution (milliseconds)"
          - "Test single units in isolation"
          - "No external dependencies"
          - "Highest quantity, lowest cost"
      integration_tests:
        percentage: "20%"
        characteristics:
          - "Medium speed (seconds)"
          - "Test component interactions"
          - "May use test databases, queues"
          - "Verify integration contracts"
      e2e_tests:
        percentage: "10%"
        characteristics:
          - "Slowest execution (minutes)"
          - "Test complete user journeys"
          - "Real browser/API interactions"
          - "Highest confidence, highest cost"

    anti_patterns:
      ice_cream_cone:
        description: "Inverted pyramid - too many E2E, few unit tests"
        problems:
          - "Slow CI/CD pipelines (30+ minutes)"
          - "Flaky tests due to environment complexity"
          - "Difficult to pinpoint failure causes"
          - "High maintenance burden"
      
      hourglass:
        description: "Many unit and E2E, missing integration tests"
        problems:
          - "Integration bugs escape to production"
          - "Components work alone but fail together"
          - "False confidence from passing tests"

  best_practices:
    good_examples:
      - pattern: "Test at the right level"
        description: "Push tests down the pyramid when possible"
        example: |
          # BAD: E2E test for business logic
          def test_discount_calculation_e2e():
              browser.goto("/checkout")
              browser.fill("#coupon", "SAVE20")
              browser.click("#apply")
              assert browser.text("#total") == "$80.00"  # Slow, flaky
          
          # GOOD: Unit test for business logic
          def test_discount_calculation():
              cart = Cart(items=[Item(price=100)])
              cart.apply_coupon("SAVE20")
              assert cart.total == 80.00  # Fast, reliable
      
      - pattern: "Clear test boundaries"
        description: "Each layer tests specific concerns"
        example: |
          # Unit: Business logic
          def test_order_validates_stock():
              order = Order(product_id=1, quantity=10)
              with pytest.raises(InsufficientStockError):
                  order.validate(available_stock=5)
          
          # Integration: Database interaction
          def test_order_persists_correctly(db_session):
              order = create_order(product_id=1, quantity=5)
              saved = db_session.query(Order).filter_by(id=order.id).first()
              assert saved.quantity == 5
              assert saved.status == "pending"
          
          # E2E: User journey
          def test_complete_purchase_flow(page):
              page.goto("/products/1")
              page.click("[data-testid='add-to-cart']")
              page.click("[data-testid='checkout']")
              page.fill("#card-number", "4242424242424242")
              page.click("[data-testid='pay']")
              expect(page.locator(".confirmation")).to_be_visible()

    bad_examples:
      - pattern: "Testing implementation details"
        description: "Tests break when refactoring without behavior change"
        example: |
          # BAD: Testing private method
          def test_internal_calculation():
              calc = PriceCalculator()
              result = calc._apply_tax_rate(100, 0.1)  # Private method
              assert result == 110
          
          # GOOD: Testing public behavior
          def test_price_includes_tax():
              calc = PriceCalculator(tax_rate=0.1)
              assert calc.calculate_total(100) == 110

      - pattern: "Overlapping test coverage"
        description: "Same logic tested at multiple levels"
        example: |
          # BAD: Redundant testing
          def test_email_validation_unit():
              assert is_valid_email("test@example.com")
          
          def test_email_validation_integration():
              user = User(email="test@example.com")
              db.save(user)  # Just testing the same validation again
          
          def test_email_validation_e2e():
              page.fill("#email", "test@example.com")
              # Still testing the same validation...

  test_classification:
    description: "Criteria for categorizing tests"
    criteria:
      unit_test:
        - "Executes in < 100ms"
        - "No I/O operations (disk, network, database)"
        - "No sleep/wait statements"
        - "Single class/function focus"
        - "Uses mocks for dependencies"
      
      integration_test:
        - "Tests component boundaries"
        - "May use test containers (database, Redis)"
        - "Verifies data serialization/deserialization"
        - "Tests error handling across components"
      
      e2e_test:
        - "Simulates real user behavior"
        - "Uses production-like environment"
        - "Tests complete feature workflows"
        - "Includes UI and API interactions"


# =============================================================================
# SECTION 2: TEST AUTOMATION WITH PYTEST
# =============================================================================
pytest_automation:
  overview: |
    Pytest is the de-facto Python testing framework, offering powerful fixtures,
    parametrization, and plugin ecosystem. Master these patterns for maintainable,
    scalable test suites.

  project_structure:
    description: "Recommended test organization"
    layout: |
      project/
      ├── src/
      │   └── myapp/
      │       ├── __init__.py
      │       ├── models.py
      │       ├── services.py
      │       └── api.py
      ├── tests/
      │   ├── conftest.py           # Shared fixtures
      │   ├── pytest.ini            # Pytest configuration
      │   ├── unit/
      │   │   ├── conftest.py       # Unit-specific fixtures
      │   │   ├── test_models.py
      │   │   └── test_services.py
      │   ├── integration/
      │   │   ├── conftest.py       # Integration fixtures (DB, etc.)
      │   │   └── test_api.py
      │   └── e2e/
      │       ├── conftest.py       # Browser fixtures
      │       └── test_user_flows.py
      └── pyproject.toml

  pytest_ini_config:
    description: "Production pytest configuration"
    code_example: |
      # pyproject.toml
      [tool.pytest.ini_options]
      minversion = "7.0"
      addopts = [
          "-ra",                    # Show summary of all except passed
          "-q",                     # Quieter output
          "--strict-markers",       # Error on unknown markers
          "--strict-config",        # Error on config issues
          "-p no:cacheprovider",    # Disable cache in CI
      ]
      testpaths = ["tests"]
      python_files = ["test_*.py", "*_test.py"]
      python_classes = ["Test*"]
      python_functions = ["test_*"]
      
      markers = [
          "unit: Unit tests (fast, no I/O)",
          "integration: Integration tests (database, external services)",
          "e2e: End-to-end tests (browser, full stack)",
          "slow: Tests that take > 1 second",
          "smoke: Critical path tests for deployment validation",
      ]
      
      filterwarnings = [
          "error",                  # Treat warnings as errors
          "ignore::DeprecationWarning:third_party.*",
      ]
      
      # Coverage configuration
      [tool.coverage.run]
      source = ["src"]
      branch = true
      omit = ["*/tests/*", "*/__init__.py"]
      
      [tool.coverage.report]
      exclude_lines = [
          "pragma: no cover",
          "if TYPE_CHECKING:",
          "raise NotImplementedError",
      ]
      fail_under = 80

  fixtures:
    basic_fixtures:
      description: "Fixture fundamentals and scopes"
      code_example: |
        import pytest
        from typing import Generator
        
        # Function scope (default) - runs for each test
        @pytest.fixture
        def sample_user() -> User:
            """Fresh user for each test."""
            return User(name="Test User", email="test@example.com")
        
        # Module scope - shared across tests in a module
        @pytest.fixture(scope="module")
        def expensive_resource() -> Generator[Resource, None, None]:
            """Resource created once per module."""
            resource = Resource.create()
            yield resource
            resource.cleanup()
        
        # Session scope - shared across entire test session
        @pytest.fixture(scope="session")
        def database_connection() -> Generator[Connection, None, None]:
            """Single DB connection for all tests."""
            conn = create_connection()
            yield conn
            conn.close()
        
        # Class scope - shared across test class
        @pytest.fixture(scope="class")
        def api_client(request) -> APIClient:
            """Shared client for test class."""
            client = APIClient()
            request.cls.client = client  # Attach to class
            return client

    fixture_factories:
      description: "Factory pattern for flexible test data"
      code_example: |
        import pytest
        from typing import Callable, Any
        from dataclasses import dataclass
        
        @dataclass
        class User:
            id: int
            name: str
            email: str
            role: str = "user"
        
        @pytest.fixture
        def user_factory() -> Callable[..., User]:
            """
            Factory fixture for creating users with custom attributes.
            Usage: user = user_factory(name="Custom", role="admin")
            """
            created_users = []
            counter = 0
            
            def _create_user(**overrides) -> User:
                nonlocal counter
                counter += 1
                defaults = {
                    "id": counter,
                    "name": f"User {counter}",
                    "email": f"user{counter}@test.com",
                    "role": "user",
                }
                defaults.update(overrides)
                user = User(**defaults)
                created_users.append(user)
                return user
            
            yield _create_user
            
            # Cleanup after test
            for user in created_users:
                # Delete from DB if needed
                pass
        
        # Usage in tests
        def test_user_permissions(user_factory):
            admin = user_factory(role="admin")
            regular = user_factory(role="user")
            
            assert admin.can_delete_users()
            assert not regular.can_delete_users()

    database_fixtures:
      description: "Database fixtures with transaction rollback"
      code_example: |
        import pytest
        from sqlalchemy import create_engine
        from sqlalchemy.orm import sessionmaker, Session
        
        @pytest.fixture(scope="session")
        def engine():
            """Create test database engine."""
            engine = create_engine(
                "postgresql://test:test@localhost/test_db",
                echo=False,
                pool_pre_ping=True,
            )
            yield engine
            engine.dispose()
        
        @pytest.fixture(scope="session")
        def tables(engine):
            """Create all tables once per session."""
            Base.metadata.create_all(engine)
            yield
            Base.metadata.drop_all(engine)
        
        @pytest.fixture
        def db_session(engine, tables) -> Generator[Session, None, None]:
            """
            Transactional test session - rolls back after each test.
            This ensures test isolation without data cleanup.
            """
            connection = engine.connect()
            transaction = connection.begin()
            session = sessionmaker(bind=connection)()
            
            # Begin a nested transaction (savepoint)
            nested = connection.begin_nested()
            
            # Restart savepoint after each commit
            @event.listens_for(session, "after_transaction_end")
            def restart_savepoint(session, transaction):
                nonlocal nested
                if transaction.nested and not transaction._parent.nested:
                    nested = connection.begin_nested()
            
            yield session
            
            session.close()
            transaction.rollback()
            connection.close()

    async_fixtures:
      description: "Async fixtures with pytest-asyncio"
      code_example: |
        import pytest
        import pytest_asyncio
        from httpx import AsyncClient
        
        @pytest_asyncio.fixture
        async def async_client() -> AsyncGenerator[AsyncClient, None]:
            """Async HTTP client fixture."""
            async with AsyncClient(base_url="http://test") as client:
                yield client
        
        @pytest_asyncio.fixture(scope="session")
        async def async_db_pool():
            """Async database connection pool."""
            pool = await asyncpg.create_pool(
                "postgresql://test:test@localhost/test_db",
                min_size=2,
                max_size=10,
            )
            yield pool
            await pool.close()
        
        @pytest.mark.asyncio
        async def test_async_endpoint(async_client):
            response = await async_client.get("/api/users")
            assert response.status_code == 200

  mocking:
    basic_mocking:
      description: "Essential mocking patterns"
      code_example: |
        import pytest
        from unittest.mock import Mock, MagicMock, patch, AsyncMock
        
        # Simple mock
        def test_service_calls_repository():
            mock_repo = Mock()
            mock_repo.get_user.return_value = User(id=1, name="Test")
            
            service = UserService(repository=mock_repo)
            user = service.get_user(1)
            
            mock_repo.get_user.assert_called_once_with(1)
            assert user.name == "Test"
        
        # Patching
        @patch("myapp.services.external_api.fetch_data")
        def test_with_patched_external_api(mock_fetch):
            mock_fetch.return_value = {"status": "ok"}
            
            result = process_external_data()
            
            assert result.status == "ok"
            mock_fetch.assert_called_once()
        
        # Context manager patching
        def test_with_context_patch():
            with patch.object(EmailService, "send") as mock_send:
                mock_send.return_value = True
                
                result = register_user(email="test@test.com")
                
                assert result.email_sent
                mock_send.assert_called_with(
                    to="test@test.com",
                    subject="Welcome!"
                )
        
        # Async mocking
        @pytest.mark.asyncio
        async def test_async_service():
            mock_client = AsyncMock()
            mock_client.fetch.return_value = {"data": "test"}
            
            service = AsyncDataService(client=mock_client)
            result = await service.get_data()
            
            assert result == {"data": "test"}

    advanced_mocking:
      description: "Complex mocking scenarios"
      code_example: |
        from unittest.mock import Mock, patch, call, PropertyMock
        
        # Side effects for sequential calls
        def test_retry_logic():
            mock_api = Mock()
            mock_api.call.side_effect = [
                ConnectionError("Failed"),
                ConnectionError("Failed"),
                {"status": "success"},  # Third call succeeds
            ]
            
            service = RetryingService(api=mock_api, max_retries=3)
            result = service.fetch()
            
            assert result == {"status": "success"}
            assert mock_api.call.call_count == 3
        
        # Dynamic side effects
        def test_conditional_responses():
            def dynamic_response(user_id):
                if user_id == 1:
                    return User(id=1, name="Admin")
                raise UserNotFoundError()
            
            mock_repo = Mock()
            mock_repo.get.side_effect = dynamic_response
            
            assert mock_repo.get(1).name == "Admin"
            with pytest.raises(UserNotFoundError):
                mock_repo.get(999)
        
        # Mocking properties
        def test_with_property_mock():
            with patch.object(
                Config, "api_key", new_callable=PropertyMock
            ) as mock_key:
                mock_key.return_value = "test-key-123"
                
                client = APIClient()
                assert client.config.api_key == "test-key-123"
        
        # Verifying call order
        def test_operation_sequence():
            mock_db = Mock()
            
            with TransactionManager(mock_db) as tx:
                tx.insert({"data": "test"})
                tx.commit()
            
            expected_calls = [
                call.begin(),
                call.insert({"data": "test"}),
                call.commit(),
                call.close(),
            ]
            mock_db.assert_has_calls(expected_calls, any_order=False)

    pytest_mock_plugin:
      description: "Using pytest-mock for cleaner mocking"
      code_example: |
        import pytest
        
        def test_with_mocker(mocker):
            """pytest-mock provides 'mocker' fixture."""
            # Patch with automatic cleanup
            mock_send = mocker.patch("myapp.email.send_email")
            mock_send.return_value = True
            
            # Spy on real implementation
            spy_log = mocker.spy(logger, "info")
            
            register_user("test@test.com")
            
            mock_send.assert_called_once()
            spy_log.assert_called_with("User registered: test@test.com")
        
        def test_mock_datetime(mocker):
            """Freeze time for deterministic tests."""
            mock_now = mocker.patch("myapp.utils.datetime")
            mock_now.now.return_value = datetime(2026, 1, 1, 12, 0, 0)
            
            event = create_event()
            
            assert event.created_at == datetime(2026, 1, 1, 12, 0, 0)

  parametrization:
    description: "Data-driven testing with parametrize"
    code_example: |
      import pytest
      
      # Basic parametrization
      @pytest.mark.parametrize("input,expected", [
          ("hello", "HELLO"),
          ("World", "WORLD"),
          ("pytest", "PYTEST"),
          ("", ""),
      ])
      def test_uppercase(input, expected):
          assert input.upper() == expected
      
      # Multiple parameter sets with IDs
      @pytest.mark.parametrize(
          "email,is_valid",
          [
              pytest.param("user@example.com", True, id="valid-standard"),
              pytest.param("user+tag@example.com", True, id="valid-plus"),
              pytest.param("user@sub.example.com", True, id="valid-subdomain"),
              pytest.param("invalid", False, id="invalid-no-at"),
              pytest.param("@example.com", False, id="invalid-no-user"),
              pytest.param("user@", False, id="invalid-no-domain"),
          ],
      )
      def test_email_validation(email, is_valid):
          assert validate_email(email) == is_valid
      
      # Combining parameters (cartesian product)
      @pytest.mark.parametrize("x", [1, 2, 3])
      @pytest.mark.parametrize("y", [10, 20])
      def test_multiplication(x, y):
          # Runs 6 times: (1,10), (1,20), (2,10), (2,20), (3,10), (3,20)
          assert x * y == x * y
      
      # Parametrize with fixtures
      @pytest.fixture(params=["sqlite", "postgresql", "mysql"])
      def database(request):
          """Test against multiple databases."""
          db = create_database(request.param)
          yield db
          db.cleanup()
      
      def test_crud_operations(database):
          # This test runs 3 times, once per database
          database.insert({"id": 1, "name": "test"})
          assert database.get(1)["name"] == "test"

  best_practices:
    good_examples:
      - pattern: "Arrange-Act-Assert (AAA)"
        description: "Clear test structure"
        example: |
          def test_user_registration():
              # Arrange
              email = "new@user.com"
              password = "SecurePass123!"
              
              # Act
              user = register_user(email, password)
              
              # Assert
              assert user.id is not None
              assert user.email == email
              assert user.is_active is False  # Pending verification
      
      - pattern: "Single assertion concept"
        description: "Test one behavior, may have multiple asserts"
        example: |
          def test_order_creation_sets_correct_defaults():
              order = Order.create(product_id=1, quantity=2)
              
              # Multiple asserts for ONE concept: default values
              assert order.status == "pending"
              assert order.created_at is not None
              assert order.shipping_address is None
              assert order.total == 0  # Calculated later
      
      - pattern: "Descriptive test names"
        description: "Test name describes behavior and expectation"
        example: |
          # BAD
          def test_order():
              ...
          
          def test_order_2():
              ...
          
          # GOOD
          def test_order_with_invalid_quantity_raises_validation_error():
              ...
          
          def test_order_total_includes_tax_when_applicable():
              ...
          
          def test_cancelled_order_cannot_be_shipped():
              ...

    bad_examples:
      - pattern: "Tests with external dependencies"
        description: "Unit tests should not hit real services"
        example: |
          # BAD: Calls real API
          def test_weather_service():
              service = WeatherService()
              weather = service.get_weather("New York")
              assert weather["temp"] > -50  # Flaky, depends on real data
          
          # GOOD: Mocked dependency
          def test_weather_service(mocker):
              mock_api = mocker.patch("myapp.weather.api.fetch")
              mock_api.return_value = {"temp": 72, "condition": "sunny"}
              
              service = WeatherService()
              weather = service.get_weather("New York")
              
              assert weather["temp"] == 72
      
      - pattern: "Test interdependence"
        description: "Tests should not depend on execution order"
        example: |
          # BAD: Tests share state
          class TestUserCRUD:
              user_id = None
              
              def test_create_user(self, db):
                  user = db.create_user(name="Test")
                  TestUserCRUD.user_id = user.id  # Shared state!
              
              def test_get_user(self, db):
                  user = db.get_user(TestUserCRUD.user_id)  # Depends on previous test
                  assert user.name == "Test"
          
          # GOOD: Independent tests
          def test_create_user(db):
              user = db.create_user(name="Test")
              assert user.id is not None
          
          def test_get_user(db, user_factory):
              user = user_factory()  # Creates its own user
              fetched = db.get_user(user.id)
              assert fetched.name == user.name


# =============================================================================
# SECTION 3: E2E TESTING WITH PLAYWRIGHT
# =============================================================================
e2e_testing:
  overview: |
    End-to-end tests validate complete user journeys through the application.
    Playwright offers reliable, cross-browser automation with modern async API
    and powerful debugging capabilities.

  playwright_setup:
    description: "Production Playwright configuration"
    code_example: |
      # playwright.config.ts (or pytest-playwright conftest.py)
      import pytest
      from playwright.sync_api import Playwright
      
      # conftest.py
      @pytest.fixture(scope="session")
      def browser_context_args(browser_context_args):
          """Configure browser context."""
          return {
              **browser_context_args,
              "viewport": {"width": 1920, "height": 1080},
              "ignore_https_errors": True,
              "record_video_dir": "test-results/videos",
              "record_har_path": "test-results/har/trace.har",
          }
      
      @pytest.fixture(scope="session")
      def browser_type_launch_args(browser_type_launch_args):
          """Configure browser launch."""
          return {
              **browser_type_launch_args,
              "headless": True,
              "slow_mo": 0,  # Set to 100+ for debugging
          }

  page_object_model:
    description: "Page Object pattern for maintainable E2E tests"
    code_example: |
      # pages/base_page.py
      from playwright.sync_api import Page, expect
      from typing import Self
      
      class BasePage:
          """Base page with common functionality."""
          
          def __init__(self, page: Page):
              self.page = page
          
          def navigate(self, path: str = "") -> Self:
              """Navigate to page path."""
              self.page.goto(f"{self.base_url}{path}")
              return self
          
          def wait_for_load(self) -> Self:
              """Wait for page to be fully loaded."""
              self.page.wait_for_load_state("networkidle")
              return self
          
          def take_screenshot(self, name: str) -> Self:
              """Capture screenshot for debugging."""
              self.page.screenshot(path=f"screenshots/{name}.png")
              return self
      
      # pages/login_page.py
      from pages.base_page import BasePage
      
      class LoginPage(BasePage):
          """Login page object."""
          
          base_url = "/login"
          
          # Locators (centralized, easy to update)
          SELECTORS = {
              "email_input": "[data-testid='email-input']",
              "password_input": "[data-testid='password-input']",
              "submit_button": "[data-testid='login-submit']",
              "error_message": "[data-testid='login-error']",
              "forgot_password": "a:has-text('Forgot password?')",
          }
          
          def login(self, email: str, password: str) -> "DashboardPage":
              """Perform login and return dashboard page."""
              self.page.fill(self.SELECTORS["email_input"], email)
              self.page.fill(self.SELECTORS["password_input"], password)
              self.page.click(self.SELECTORS["submit_button"])
              
              # Wait for navigation
              self.page.wait_for_url("**/dashboard")
              
              from pages.dashboard_page import DashboardPage
              return DashboardPage(self.page)
          
          def login_expecting_error(self, email: str, password: str) -> Self:
              """Attempt login expecting failure."""
              self.page.fill(self.SELECTORS["email_input"], email)
              self.page.fill(self.SELECTORS["password_input"], password)
              self.page.click(self.SELECTORS["submit_button"])
              return self
          
          def get_error_message(self) -> str:
              """Get displayed error message."""
              error = self.page.locator(self.SELECTORS["error_message"])
              expect(error).to_be_visible()
              return error.text_content()
          
          def click_forgot_password(self) -> "ForgotPasswordPage":
              """Navigate to forgot password page."""
              self.page.click(self.SELECTORS["forgot_password"])
              from pages.forgot_password_page import ForgotPasswordPage
              return ForgotPasswordPage(self.page)
      
      # pages/dashboard_page.py
      class DashboardPage(BasePage):
          """Dashboard page object."""
          
          SELECTORS = {
              "welcome_message": "[data-testid='welcome-message']",
              "user_menu": "[data-testid='user-menu']",
              "logout_button": "[data-testid='logout']",
              "notification_badge": "[data-testid='notifications'] .badge",
          }
          
          def get_welcome_message(self) -> str:
              return self.page.locator(self.SELECTORS["welcome_message"]).text_content()
          
          def logout(self) -> LoginPage:
              self.page.click(self.SELECTORS["user_menu"])
              self.page.click(self.SELECTORS["logout_button"])
              return LoginPage(self.page)
          
          def get_notification_count(self) -> int:
              badge = self.page.locator(self.SELECTORS["notification_badge"])
              if badge.is_visible():
                  return int(badge.text_content())
              return 0

    page_object_test_example:
      description: "Using page objects in tests"
      code_example: |
        import pytest
        from pages.login_page import LoginPage
        
        class TestAuthentication:
            """Authentication E2E tests."""
            
            def test_successful_login(self, page, test_user):
                """User can log in with valid credentials."""
                login_page = LoginPage(page).navigate()
                
                dashboard = login_page.login(
                    email=test_user.email,
                    password=test_user.password
                )
                
                assert "Welcome" in dashboard.get_welcome_message()
                assert test_user.name in dashboard.get_welcome_message()
            
            def test_invalid_credentials_shows_error(self, page):
                """Invalid credentials display error message."""
                login_page = LoginPage(page).navigate()
                
                login_page.login_expecting_error(
                    email="wrong@email.com",
                    password="wrongpassword"
                )
                
                assert login_page.get_error_message() == "Invalid email or password"
            
            def test_logout_redirects_to_login(self, page, authenticated_page):
                """Logging out returns user to login page."""
                dashboard = DashboardPage(authenticated_page)
                
                login_page = dashboard.logout()
                
                assert "/login" in page.url

  component_testing:
    description: "Testing reusable UI components"
    code_example: |
      # components/data_table.py
      from playwright.sync_api import Page, Locator
      
      class DataTable:
          """Reusable data table component."""
          
          def __init__(self, page: Page, container_selector: str):
              self.page = page
              self.container = page.locator(container_selector)
          
          @property
          def rows(self) -> Locator:
              return self.container.locator("tbody tr")
          
          @property
          def headers(self) -> list[str]:
              return self.container.locator("thead th").all_text_contents()
          
          def row_count(self) -> int:
              return self.rows.count()
          
          def get_cell(self, row: int, column: int) -> str:
              return self.rows.nth(row).locator("td").nth(column).text_content()
          
          def get_row_data(self, row: int) -> dict:
              cells = self.rows.nth(row).locator("td").all_text_contents()
              return dict(zip(self.headers, cells))
          
          def sort_by(self, column_name: str) -> None:
              header = self.container.locator(f"th:has-text('{column_name}')")
              header.click()
              self.page.wait_for_load_state("networkidle")
          
          def filter_by(self, column: str, value: str) -> None:
              filter_input = self.container.locator(
                  f"[data-filter-column='{column}']"
              )
              filter_input.fill(value)
              filter_input.press("Enter")
              self.page.wait_for_load_state("networkidle")
          
          def select_row(self, row: int) -> None:
              checkbox = self.rows.nth(row).locator("input[type='checkbox']")
              checkbox.check()
          
          def get_selected_rows(self) -> list[int]:
              selected = []
              for i in range(self.row_count()):
                  checkbox = self.rows.nth(i).locator("input[type='checkbox']")
                  if checkbox.is_checked():
                      selected.append(i)
              return selected

  playwright_patterns:
    waiting_strategies:
      description: "Reliable waiting patterns"
      code_example: |
        from playwright.sync_api import Page, expect
        
        class WaitingPatterns:
            """Best practices for waiting in Playwright."""
            
            def wait_for_element(self, page: Page):
                # Auto-waiting (preferred) - Playwright waits automatically
                page.click("button")  # Waits for button to be clickable
                
                # Explicit visibility wait
                page.locator(".loading-spinner").wait_for(state="hidden")
                
                # Wait for specific condition
                expect(page.locator(".results")).to_have_count(10)
                
                # Custom wait with polling
                page.wait_for_function(
                    "document.querySelectorAll('.item').length >= 5"
                )
            
            def wait_for_api_response(self, page: Page):
                # Wait for specific API call
                with page.expect_response("**/api/users") as response_info:
                    page.click("#load-users")
                response = response_info.value
                assert response.status == 200
                
                # Wait for multiple requests
                with page.expect_response(
                    lambda r: "/api/" in r.url and r.status == 200
                ):
                    page.click("#submit")
            
            def wait_for_navigation(self, page: Page):
                # Wait for URL change
                with page.expect_navigation():
                    page.click("a.nav-link")
                
                # Wait for specific URL
                page.wait_for_url("**/dashboard")
            
            # ANTI-PATTERNS - Never use these
            def bad_waiting(self, page: Page):
                # BAD: Fixed sleep
                import time
                time.sleep(2)  # Flaky, slow
                
                # BAD: Arbitrary timeout
                page.wait_for_timeout(5000)  # Same issues

    network_interception:
      description: "Mock API responses for reliable testing"
      code_example: |
        def test_with_mocked_api(page):
            """Mock API responses for controlled testing."""
            
            # Mock specific endpoint
            page.route(
                "**/api/users",
                lambda route: route.fulfill(
                    status=200,
                    content_type="application/json",
                    body='[{"id": 1, "name": "Test User"}]'
                )
            )
            
            page.goto("/users")
            expect(page.locator(".user-card")).to_have_count(1)
        
        def test_api_error_handling(page):
            """Test UI handles API errors gracefully."""
            
            page.route(
                "**/api/users",
                lambda route: route.fulfill(status=500, body="Server Error")
            )
            
            page.goto("/users")
            expect(page.locator(".error-message")).to_be_visible()
            expect(page.locator(".error-message")).to_have_text(
                "Failed to load users. Please try again."
            )
        
        def test_slow_network(page):
            """Test loading states with delayed responses."""
            
            def delayed_response(route):
                import time
                time.sleep(2)  # Simulate slow network
                route.fulfill(
                    status=200,
                    body='{"data": "loaded"}'
                )
            
            page.route("**/api/data", delayed_response)
            
            page.goto("/data")
            
            # Verify loading state appears
            expect(page.locator(".loading-spinner")).to_be_visible()
            
            # Verify data eventually loads
            expect(page.locator(".data-content")).to_be_visible(timeout=5000)

    visual_testing:
      description: "Screenshot comparison testing"
      code_example: |
        import pytest
        
        def test_homepage_visual(page):
            """Visual regression test for homepage."""
            page.goto("/")
            page.wait_for_load_state("networkidle")
            
            # Full page screenshot comparison
            expect(page).to_have_screenshot(
                "homepage.png",
                full_page=True,
                threshold=0.1,  # 10% difference tolerance
            )
        
        def test_component_visual(page):
            """Visual test for specific component."""
            page.goto("/components")
            
            # Component-specific screenshot
            card = page.locator(".feature-card").first
            expect(card).to_have_screenshot("feature-card.png")
        
        def test_responsive_layouts(page):
            """Test responsive design at different viewports."""
            viewports = [
                {"width": 375, "height": 667, "name": "mobile"},
                {"width": 768, "height": 1024, "name": "tablet"},
                {"width": 1920, "height": 1080, "name": "desktop"},
            ]
            
            for viewport in viewports:
                page.set_viewport_size({
                    "width": viewport["width"],
                    "height": viewport["height"]
                })
                page.goto("/")
                expect(page).to_have_screenshot(
                    f"homepage-{viewport['name']}.png"
                )

  cypress_patterns:
    description: "Cypress-specific patterns for JavaScript/TypeScript projects"
    code_example: |
      // cypress/support/commands.ts
      Cypress.Commands.add('login', (email: string, password: string) => {
        cy.session([email, password], () => {
          cy.visit('/login')
          cy.get('[data-testid="email"]').type(email)
          cy.get('[data-testid="password"]').type(password)
          cy.get('[data-testid="submit"]').click()
          cy.url().should('include', '/dashboard')
        })
      })
      
      Cypress.Commands.add('interceptApi', (method: string, url: string, fixture: string) => {
        cy.intercept(method, url, { fixture }).as('apiCall')
      })
      
      // cypress/e2e/checkout.cy.ts
      describe('Checkout Flow', () => {
        beforeEach(() => {
          cy.login('test@example.com', 'password123')
        })
        
        it('completes checkout with valid payment', () => {
          // Arrange
          cy.interceptApi('POST', '/api/orders', 'order-success.json')
          cy.interceptApi('POST', '/api/payments', 'payment-success.json')
          
          // Act
          cy.visit('/cart')
          cy.get('[data-testid="checkout-btn"]').click()
          cy.get('[data-testid="card-number"]').type('4242424242424242')
          cy.get('[data-testid="expiry"]').type('12/28')
          cy.get('[data-testid="cvc"]').type('123')
          cy.get('[data-testid="pay-btn"]').click()
          
          // Assert
          cy.wait('@apiCall')
          cy.url().should('include', '/confirmation')
          cy.get('[data-testid="order-number"]').should('be.visible')
        })
        
        it('displays error for declined card', () => {
          cy.intercept('POST', '/api/payments', {
            statusCode: 400,
            body: { error: 'Card declined' }
          })
          
          cy.visit('/checkout')
          cy.get('[data-testid="card-number"]').type('4000000000000002')
          cy.get('[data-testid="pay-btn"]').click()
          
          cy.get('[data-testid="error-message"]')
            .should('be.visible')
            .and('contain', 'Card declined')
        })
      })

  best_practices:
    good_examples:
      - pattern: "Use data-testid attributes"
        description: "Stable selectors that don't break with UI changes"
        example: |
          # BAD: Fragile selectors
          page.click(".btn.btn-primary.submit-button")  # Class changes break this
          page.click("//div[3]/form/button[2]")  # XPath breaks with DOM changes
          page.click("text=Submit Order Now!")  # Text changes break this
          
          # GOOD: Stable test IDs
          page.click("[data-testid='submit-order']")  # Dedicated test attribute
          page.click("role=button[name='Submit']")  # Accessible role
      
      - pattern: "Test user behavior, not implementation"
        description: "Write tests from user perspective"
        example: |
          # BAD: Testing implementation details
          def test_login_sets_localStorage():
              page.goto("/login")
              page.fill("#email", "test@test.com")
              page.fill("#password", "pass")
              page.click("#submit")
              
              # Testing implementation detail
              token = page.evaluate("localStorage.getItem('auth_token')")
              assert token is not None
          
          # GOOD: Testing user outcome
          def test_user_can_access_dashboard_after_login():
              login_page = LoginPage(page).navigate()
              dashboard = login_page.login("test@test.com", "pass")
              
              # Test what user sees
              assert dashboard.is_displayed()
              assert "Welcome" in dashboard.get_greeting()

    bad_examples:
      - pattern: "Test pollution"
        description: "Tests affecting each other through shared state"
        example: |
          # BAD: Tests share database state
          def test_create_user(page):
              page.goto("/admin/users")
              page.click("#add-user")
              page.fill("#name", "Test User")  # Creates permanent record
              page.click("#save")
          
          def test_user_count(page):
              page.goto("/admin/users")
              # Fails if test_create_user runs first!
              assert page.locator(".user-row").count() == 5
          
          # GOOD: Isolated tests with cleanup
          @pytest.fixture
          def clean_users(db):
              db.execute("DELETE FROM users WHERE email LIKE '%@test.com'")
              yield
              db.execute("DELETE FROM users WHERE email LIKE '%@test.com'")
          
          def test_create_user(page, clean_users):
              ...


# =============================================================================
# SECTION 4: PERFORMANCE TESTING
# =============================================================================
performance_testing:
  overview: |
    Performance testing validates system behavior under load. K6 provides
    developer-friendly JavaScript scripting with powerful metrics and
    cloud integration.

  k6_fundamentals:
    basic_test:
      description: "K6 load test structure"
      code_example: |
        // load-test.js
        import http from 'k6/http';
        import { check, sleep } from 'k6';
        import { Rate, Trend } from 'k6/metrics';
        
        // Custom metrics
        const errorRate = new Rate('errors');
        const responseTime = new Trend('response_time');
        
        // Test configuration
        export const options = {
          stages: [
            { duration: '2m', target: 50 },   // Ramp up to 50 users
            { duration: '5m', target: 50 },   // Stay at 50 users
            { duration: '2m', target: 100 },  // Ramp up to 100 users
            { duration: '5m', target: 100 },  // Stay at 100 users
            { duration: '2m', target: 0 },    // Ramp down
          ],
          thresholds: {
            http_req_duration: ['p(95)<500', 'p(99)<1000'],  // 95% < 500ms
            errors: ['rate<0.01'],                           // Error rate < 1%
            http_req_failed: ['rate<0.01'],                  // Request failures < 1%
          },
        };
        
        // Setup (runs once per test)
        export function setup() {
          const loginRes = http.post('https://api.example.com/auth/login', {
            email: 'loadtest@example.com',
            password: 'testpassword',
          });
          return { token: loginRes.json('token') };
        }
        
        // Main test function (runs per virtual user iteration)
        export default function(data) {
          const headers = {
            'Authorization': `Bearer ${data.token}`,
            'Content-Type': 'application/json',
          };
          
          // GET request
          const listRes = http.get('https://api.example.com/api/products', { headers });
          
          check(listRes, {
            'status is 200': (r) => r.status === 200,
            'response time < 500ms': (r) => r.timings.duration < 500,
            'has products': (r) => r.json('products').length > 0,
          });
          
          errorRate.add(listRes.status !== 200);
          responseTime.add(listRes.timings.duration);
          
          sleep(1); // Think time between requests
        }
        
        // Teardown (runs once after test)
        export function teardown(data) {
          // Cleanup if needed
        }

    test_scenarios:
      description: "Different load testing patterns"
      code_example: |
        // scenarios.js - Multiple test scenarios
        import http from 'k6/http';
        import { sleep } from 'k6';
        
        export const options = {
          scenarios: {
            // Smoke test - minimal load for sanity check
            smoke: {
              executor: 'constant-vus',
              vus: 1,
              duration: '1m',
              tags: { test_type: 'smoke' },
            },
            
            // Load test - normal expected load
            average_load: {
              executor: 'ramping-vus',
              startVUs: 0,
              stages: [
                { duration: '5m', target: 100 },
                { duration: '30m', target: 100 },
                { duration: '5m', target: 0 },
              ],
              tags: { test_type: 'load' },
            },
            
            // Stress test - beyond normal capacity
            stress: {
              executor: 'ramping-arrival-rate',
              startRate: 10,
              timeUnit: '1s',
              preAllocatedVUs: 500,
              maxVUs: 1000,
              stages: [
                { duration: '2m', target: 50 },
                { duration: '5m', target: 50 },
                { duration: '2m', target: 100 },
                { duration: '5m', target: 100 },
                { duration: '2m', target: 200 },  // Push limits
                { duration: '5m', target: 200 },
                { duration: '5m', target: 0 },
              ],
              tags: { test_type: 'stress' },
            },
            
            // Spike test - sudden traffic surge
            spike: {
              executor: 'ramping-vus',
              startVUs: 0,
              stages: [
                { duration: '10s', target: 500 },  // Instant spike
                { duration: '1m', target: 500 },
                { duration: '10s', target: 0 },    // Instant drop
              ],
              tags: { test_type: 'spike' },
            },
            
            // Soak test - extended duration
            soak: {
              executor: 'constant-vus',
              vus: 50,
              duration: '4h',
              tags: { test_type: 'soak' },
            },
          },
        };

    realistic_user_flows:
      description: "Simulating realistic user behavior"
      code_example: |
        // user-journey.js
        import http from 'k6/http';
        import { check, group, sleep } from 'k6';
        import { randomItem, randomIntBetween } from 'https://jslib.k6.io/k6-utils/1.2.0/index.js';
        
        const BASE_URL = 'https://api.example.com';
        
        export default function() {
          let token;
          
          // User journey simulation
          group('Authentication', () => {
            const loginRes = http.post(`${BASE_URL}/auth/login`, JSON.stringify({
              email: 'user@example.com',
              password: 'password123',
            }), {
              headers: { 'Content-Type': 'application/json' },
            });
            
            check(loginRes, {
              'login successful': (r) => r.status === 200,
            });
            
            token = loginRes.json('token');
          });
          
          sleep(randomIntBetween(1, 3)); // Realistic think time
          
          const headers = {
            'Authorization': `Bearer ${token}`,
            'Content-Type': 'application/json',
          };
          
          group('Browse Products', () => {
            // View category
            const categoriesRes = http.get(`${BASE_URL}/api/categories`, { headers });
            check(categoriesRes, { 'categories loaded': (r) => r.status === 200 });
            
            sleep(randomIntBetween(2, 5));
            
            // Browse products
            const category = randomItem(categoriesRes.json('categories'));
            const productsRes = http.get(
              `${BASE_URL}/api/products?category=${category.id}`,
              { headers }
            );
            check(productsRes, { 'products loaded': (r) => r.status === 200 });
            
            sleep(randomIntBetween(2, 8));
            
            // View product detail
            const product = randomItem(productsRes.json('products'));
            const detailRes = http.get(
              `${BASE_URL}/api/products/${product.id}`,
              { headers }
            );
            check(detailRes, { 'product detail loaded': (r) => r.status === 200 });
          });
          
          sleep(randomIntBetween(1, 3));
          
          group('Add to Cart', () => {
            const addRes = http.post(`${BASE_URL}/api/cart/items`, JSON.stringify({
              productId: randomIntBetween(1, 100),
              quantity: randomIntBetween(1, 3),
            }), { headers });
            
            check(addRes, { 'item added to cart': (r) => r.status === 201 });
          });
          
          sleep(randomIntBetween(5, 15)); // Longer think time before checkout
          
          // 30% of users proceed to checkout
          if (Math.random() < 0.3) {
            group('Checkout', () => {
              const checkoutRes = http.post(`${BASE_URL}/api/orders`, JSON.stringify({
                paymentMethod: 'credit_card',
              }), { headers });
              
              check(checkoutRes, { 'order created': (r) => r.status === 201 });
            });
          }
        }

  locust_alternative:
    description: "Python-based load testing with Locust"
    code_example: |
      # locustfile.py
      from locust import HttpUser, task, between, events
      import random
      
      class WebsiteUser(HttpUser):
          """Simulates user behavior on the website."""
          
          wait_time = between(1, 5)  # Random wait between tasks
          
          def on_start(self):
              """Called when user starts - login."""
              response = self.client.post("/auth/login", json={
                  "email": f"user{random.randint(1, 1000)}@test.com",
                  "password": "testpass123"
              })
              self.token = response.json().get("token")
              self.client.headers = {"Authorization": f"Bearer {self.token}"}
          
          @task(10)  # Weight 10 - most common action
          def browse_products(self):
              """Browse product catalog."""
              self.client.get("/api/products")
          
          @task(5)  # Weight 5 - common action
          def view_product(self):
              """View single product detail."""
              product_id = random.randint(1, 100)
              self.client.get(f"/api/products/{product_id}")
          
          @task(3)  # Weight 3 - less common
          def add_to_cart(self):
              """Add item to cart."""
              self.client.post("/api/cart/items", json={
                  "product_id": random.randint(1, 100),
                  "quantity": random.randint(1, 3)
              })
          
          @task(1)  # Weight 1 - rare action
          def checkout(self):
              """Complete purchase."""
              self.client.post("/api/orders", json={
                  "payment_method": "credit_card"
              })
      
      # Custom metrics reporting
      @events.request.add_listener
      def on_request(request_type, name, response_time, response_length, **kwargs):
          if response_time > 1000:  # Log slow requests
              print(f"Slow request: {name} took {response_time}ms")

  performance_metrics:
    description: "Key metrics to monitor during load tests"
    metrics:
      response_time:
        - metric: "p50 (median)"
          description: "Typical user experience"
          threshold: "< 200ms for APIs, < 1s for pages"
        - metric: "p95"
          description: "Worst case for most users"
          threshold: "< 500ms for APIs, < 3s for pages"
        - metric: "p99"
          description: "Edge case performance"
          threshold: "< 1s for APIs, < 5s for pages"
      
      throughput:
        - metric: "Requests per second (RPS)"
          description: "System capacity"
          calculation: "Total requests / test duration"
        - metric: "Transactions per second (TPS)"
          description: "Complete operations per second"
      
      error_rates:
        - metric: "HTTP error rate"
          description: "4xx and 5xx responses"
          threshold: "< 1% under normal load"
        - metric: "Timeout rate"
          description: "Requests exceeding timeout"
          threshold: "< 0.1%"
      
      resource_utilization:
        - metric: "CPU usage"
          threshold: "< 70% average, < 90% peak"
        - metric: "Memory usage"
          threshold: "Stable, no leaks over time"
        - metric: "Database connections"
          threshold: "< 80% of pool size"

  best_practices:
    good_examples:
      - pattern: "Realistic load profiles"
        description: "Match production traffic patterns"
        example: |
          // GOOD: Varied user behavior
          export const options = {
            scenarios: {
              browsers: {
                executor: 'ramping-vus',
                stages: [
                  { duration: '5m', target: 80 },  // 80% browsers
                ],
                exec: 'browseProducts',
              },
              buyers: {
                executor: 'constant-arrival-rate',
                rate: 10,  // 10 purchases/sec
                timeUnit: '1s',
                duration: '10m',
                preAllocatedVUs: 50,
                exec: 'completePurchase',
              },
            },
          };
      
      - pattern: "Proper warm-up periods"
        description: "Allow systems to initialize before measuring"
        example: |
          export const options = {
            stages: [
              { duration: '2m', target: 10 },   // Warm-up (excluded from metrics)
              { duration: '5m', target: 100 },  // Ramp to target
              { duration: '10m', target: 100 }, // Measure steady state
              { duration: '2m', target: 0 },    // Cool down
            ],
          };

    bad_examples:
      - pattern: "Unrealistic scenarios"
        description: "Tests that don't match real usage"
        example: |
          // BAD: Hammer single endpoint
          export default function() {
            http.get('https://api.example.com/health');
            // No think time, no variety - not realistic
          }
          
          // GOOD: Realistic user journey with varied endpoints
          export default function() {
            http.get('/api/products');
            sleep(randomIntBetween(2, 5));
            http.get('/api/products/123');
            sleep(randomIntBetween(1, 3));
            http.post('/api/cart/items', {...});
          }


# =============================================================================
# SECTION 5: API TESTING
# =============================================================================
api_testing:
  overview: |
    API testing validates service contracts, data integrity, and error handling.
    Combines automated testing, contract verification, and schema validation
    for comprehensive API quality assurance.

  pytest_api_testing:
    description: "API testing with pytest and requests/httpx"
    code_example: |
      # tests/api/conftest.py
      import pytest
      import httpx
      from typing import Generator
      
      @pytest.fixture(scope="session")
      def api_client() -> Generator[httpx.Client, None, None]:
          """HTTP client for API tests."""
          with httpx.Client(
              base_url="http://localhost:8000",
              timeout=30.0,
              headers={"Content-Type": "application/json"}
          ) as client:
              yield client
      
      @pytest.fixture
      def auth_client(api_client, test_user) -> httpx.Client:
          """Authenticated API client."""
          response = api_client.post("/auth/login", json={
              "email": test_user.email,
              "password": test_user.password,
          })
          token = response.json()["token"]
          api_client.headers["Authorization"] = f"Bearer {token}"
          return api_client
      
      # tests/api/test_users.py
      import pytest
      from http import HTTPStatus
      
      class TestUserAPI:
          """User API endpoint tests."""
          
          def test_create_user_success(self, api_client):
              """POST /users creates new user."""
              payload = {
                  "email": "newuser@example.com",
                  "name": "New User",
                  "password": "SecurePass123!",
              }
              
              response = api_client.post("/api/users", json=payload)
              
              assert response.status_code == HTTPStatus.CREATED
              data = response.json()
              assert data["email"] == payload["email"]
              assert data["name"] == payload["name"]
              assert "id" in data
              assert "password" not in data  # Password not returned
          
          def test_create_user_duplicate_email(self, api_client, existing_user):
              """POST /users with duplicate email returns 409."""
              payload = {
                  "email": existing_user.email,  # Already exists
                  "name": "Another User",
                  "password": "SecurePass123!",
              }
              
              response = api_client.post("/api/users", json=payload)
              
              assert response.status_code == HTTPStatus.CONFLICT
              assert response.json()["error"] == "email_already_exists"
          
          def test_create_user_invalid_email(self, api_client):
              """POST /users with invalid email returns 422."""
              payload = {
                  "email": "not-an-email",
                  "name": "Test User",
                  "password": "SecurePass123!",
              }
              
              response = api_client.post("/api/users", json=payload)
              
              assert response.status_code == HTTPStatus.UNPROCESSABLE_ENTITY
              errors = response.json()["errors"]
              assert any(e["field"] == "email" for e in errors)
          
          def test_get_user_requires_auth(self, api_client):
              """GET /users/{id} without auth returns 401."""
              response = api_client.get("/api/users/1")
              
              assert response.status_code == HTTPStatus.UNAUTHORIZED
          
          def test_get_user_success(self, auth_client, test_user):
              """GET /users/{id} returns user data."""
              response = auth_client.get(f"/api/users/{test_user.id}")
              
              assert response.status_code == HTTPStatus.OK
              data = response.json()
              assert data["id"] == test_user.id
              assert data["email"] == test_user.email
          
          @pytest.mark.parametrize("field,value,error", [
              ("email", "", "email_required"),
              ("email", "x" * 256, "email_too_long"),
              ("name", "", "name_required"),
              ("password", "123", "password_too_weak"),
          ])
          def test_create_user_validation(self, api_client, field, value, error):
              """POST /users validates input fields."""
              payload = {
                  "email": "test@test.com",
                  "name": "Test",
                  "password": "ValidPass123!",
              }
              payload[field] = value
              
              response = api_client.post("/api/users", json=payload)
              
              assert response.status_code == HTTPStatus.UNPROCESSABLE_ENTITY
              assert error in str(response.json())

  contract_testing:
    description: "Contract testing with Pact"
    code_example: |
      # Consumer-side contract test
      # tests/contracts/test_user_service_consumer.py
      import pytest
      from pact import Consumer, Provider, Like, EachLike, Term
      
      @pytest.fixture(scope="session")
      def pact():
          """Set up Pact mock service."""
          pact = Consumer("WebApp").has_pact_with(
              Provider("UserService"),
              pact_dir="./pacts",
              log_dir="./logs",
          )
          pact.start_service()
          yield pact
          pact.stop_service()
      
      def test_get_user_contract(pact):
          """Contract: GET /users/{id} returns user."""
          
          # Define expected interaction
          (pact
              .given("user with ID 1 exists")
              .upon_receiving("a request for user 1")
              .with_request("GET", "/api/users/1")
              .will_respond_with(200, body={
                  "id": Like(1),
                  "email": Like("user@example.com"),
                  "name": Like("Test User"),
                  "created_at": Term(
                      r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z",
                      "2026-01-15T10:30:00Z"
                  ),
              }))
          
          # Execute request against mock
          with pact:
              client = UserServiceClient(base_url=pact.uri)
              user = client.get_user(1)
              
              assert user.id == 1
              assert "@" in user.email
          
          # Pact verifies the interaction occurred as expected
      
      def test_list_users_contract(pact):
          """Contract: GET /users returns user list."""
          
          (pact
              .given("users exist")
              .upon_receiving("a request to list users")
              .with_request("GET", "/api/users", query={"page": "1", "limit": "10"})
              .will_respond_with(200, body={
                  "users": EachLike({
                      "id": Like(1),
                      "email": Like("user@example.com"),
                      "name": Like("User Name"),
                  }),
                  "total": Like(100),
                  "page": Like(1),
                  "limit": Like(10),
              }))
          
          with pact:
              client = UserServiceClient(base_url=pact.uri)
              result = client.list_users(page=1, limit=10)
              
              assert len(result.users) > 0
              assert result.total >= 0
      
      # Provider-side verification
      # tests/contracts/test_user_service_provider.py
      from pact import Verifier
      
      def test_provider_honors_contract():
          """Verify provider honors consumer contracts."""
          verifier = Verifier(
              provider="UserService",
              provider_base_url="http://localhost:8000",
          )
          
          # Set up provider states
          verifier.set_state(
              "http://localhost:8000/pact/provider-states",
              setup_states=True
          )
          
          # Verify against pact file
          output, _ = verifier.verify_pacts(
              "./pacts/webapp-userservice.json",
              enable_pending=True,
              publish_verification_results=True,
          )
          
          assert output == 0  # Verification passed

  schema_validation:
    description: "JSON Schema validation for API responses"
    code_example: |
      # schemas/user.py
      USER_SCHEMA = {
          "type": "object",
          "required": ["id", "email", "name", "created_at"],
          "properties": {
              "id": {"type": "integer", "minimum": 1},
              "email": {"type": "string", "format": "email"},
              "name": {"type": "string", "minLength": 1, "maxLength": 100},
              "created_at": {"type": "string", "format": "date-time"},
              "updated_at": {"type": ["string", "null"], "format": "date-time"},
          },
          "additionalProperties": False,
      }
      
      USER_LIST_SCHEMA = {
          "type": "object",
          "required": ["users", "total", "page", "limit"],
          "properties": {
              "users": {
                  "type": "array",
                  "items": USER_SCHEMA,
              },
              "total": {"type": "integer", "minimum": 0},
              "page": {"type": "integer", "minimum": 1},
              "limit": {"type": "integer", "minimum": 1, "maximum": 100},
          },
      }
      
      # tests/api/test_schema_validation.py
      import pytest
      import jsonschema
      from schemas.user import USER_SCHEMA, USER_LIST_SCHEMA
      
      class TestUserAPISchemas:
          """Validate API responses match schemas."""
          
          def test_get_user_response_schema(self, auth_client, test_user):
              """GET /users/{id} response matches schema."""
              response = auth_client.get(f"/api/users/{test_user.id}")
              
              assert response.status_code == 200
              
              # Validate against schema
              jsonschema.validate(response.json(), USER_SCHEMA)
          
          def test_list_users_response_schema(self, auth_client):
              """GET /users response matches schema."""
              response = auth_client.get("/api/users")
              
              assert response.status_code == 200
              jsonschema.validate(response.json(), USER_LIST_SCHEMA)
          
          def test_error_response_schema(self, api_client):
              """Error responses match error schema."""
              ERROR_SCHEMA = {
                  "type": "object",
                  "required": ["error", "message"],
                  "properties": {
                      "error": {"type": "string"},
                      "message": {"type": "string"},
                      "details": {"type": "object"},
                  },
              }
              
              response = api_client.get("/api/users/99999")
              
              assert response.status_code == 404
              jsonschema.validate(response.json(), ERROR_SCHEMA)
      
      # OpenAPI schema validation
      from openapi_core import create_spec
      from openapi_core.validation.request import RequestValidator
      from openapi_core.validation.response import ResponseValidator
      
      @pytest.fixture
      def openapi_spec():
          """Load OpenAPI specification."""
          import yaml
          with open("openapi.yaml") as f:
              spec_dict = yaml.safe_load(f)
          return create_spec(spec_dict)
      
      def test_response_matches_openapi(auth_client, openapi_spec):
          """API response matches OpenAPI specification."""
          validator = ResponseValidator(openapi_spec)
          
          response = auth_client.get("/api/users")
          
          result = validator.validate(response)
          assert not result.errors, f"Schema errors: {result.errors}"

  postman_newman:
    description: "API testing with Postman collections and Newman CLI"
    code_example: |
      // collection.json - Postman collection
      {
        "info": {
          "name": "User API Tests",
          "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
        },
        "variable": [
          { "key": "base_url", "value": "http://localhost:8000" },
          { "key": "token", "value": "" }
        ],
        "item": [
          {
            "name": "Authentication",
            "item": [
              {
                "name": "Login",
                "event": [
                  {
                    "listen": "test",
                    "script": {
                      "exec": [
                        "pm.test('Status is 200', () => {",
                        "    pm.response.to.have.status(200);",
                        "});",
                        "",
                        "pm.test('Response has token', () => {",
                        "    const json = pm.response.json();",
                        "    pm.expect(json.token).to.be.a('string');",
                        "    pm.collectionVariables.set('token', json.token);",
                        "});",
                        "",
                        "pm.test('Response time < 500ms', () => {",
                        "    pm.expect(pm.response.responseTime).to.be.below(500);",
                        "});"
                      ]
                    }
                  }
                ],
                "request": {
                  "method": "POST",
                  "url": "{{base_url}}/auth/login",
                  "body": {
                    "mode": "raw",
                    "raw": "{\"email\": \"test@example.com\", \"password\": \"password123\"}",
                    "options": { "raw": { "language": "json" } }
                  }
                }
              }
            ]
          },
          {
            "name": "Users",
            "item": [
              {
                "name": "Create User",
                "event": [
                  {
                    "listen": "test",
                    "script": {
                      "exec": [
                        "pm.test('Status is 201', () => {",
                        "    pm.response.to.have.status(201);",
                        "});",
                        "",
                        "pm.test('Response has user ID', () => {",
                        "    const json = pm.response.json();",
                        "    pm.expect(json.id).to.be.a('number');",
                        "    pm.collectionVariables.set('user_id', json.id);",
                        "});",
                        "",
                        "// Schema validation",
                        "const schema = {",
                        "    type: 'object',",
                        "    required: ['id', 'email', 'name'],",
                        "    properties: {",
                        "        id: { type: 'number' },",
                        "        email: { type: 'string' },",
                        "        name: { type: 'string' }",
                        "    }",
                        "};",
                        "",
                        "pm.test('Schema is valid', () => {",
                        "    pm.response.to.have.jsonSchema(schema);",
                        "});"
                      ]
                    }
                  }
                ],
                "request": {
                  "method": "POST",
                  "url": "{{base_url}}/api/users",
                  "header": [
                    { "key": "Authorization", "value": "Bearer {{token}}" }
                  ],
                  "body": {
                    "mode": "raw",
                    "raw": "{\"email\": \"{{$randomEmail}}\", \"name\": \"{{$randomFullName}}\", \"password\": \"SecurePass123!\"}",
                    "options": { "raw": { "language": "json" } }
                  }
                }
              }
            ]
          }
        ]
      }
      
      # Run with Newman CLI
      # newman run collection.json \
      #   --environment env.json \
      #   --reporters cli,junit \
      #   --reporter-junit-export results.xml \
      #   --iteration-count 10

  best_practices:
    good_examples:
      - pattern: "Test all status codes"
        description: "Cover success, client errors, and server errors"
        example: |
          class TestDeleteUser:
              def test_delete_success(self, auth_client, user_factory):
                  user = user_factory()
                  response = auth_client.delete(f"/api/users/{user.id}")
                  assert response.status_code == 204
              
              def test_delete_not_found(self, auth_client):
                  response = auth_client.delete("/api/users/99999")
                  assert response.status_code == 404
              
              def test_delete_unauthorized(self, api_client, test_user):
                  response = api_client.delete(f"/api/users/{test_user.id}")
                  assert response.status_code == 401
              
              def test_delete_forbidden(self, auth_client, other_user):
                  # Can't delete other user's account
                  response = auth_client.delete(f"/api/users/{other_user.id}")
                  assert response.status_code == 403
      
      - pattern: "Test edge cases"
        description: "Boundary values, empty data, special characters"
        example: |
          @pytest.mark.parametrize("name", [
              "A",                    # Minimum length
              "A" * 100,              # Maximum length
              "José García",          # Unicode characters
              "O'Connor",             # Apostrophe
              "Mary-Jane",            # Hyphen
              "  John Doe  ",         # Whitespace (should trim)
          ])
          def test_user_name_variations(self, api_client, name):
              response = api_client.post("/api/users", json={
                  "email": f"test{hash(name)}@test.com",
                  "name": name,
                  "password": "ValidPass123!",
              })
              assert response.status_code == 201

    bad_examples:
      - pattern: "Hardcoded test data"
        description: "Tests fail when data changes"
        example: |
          # BAD: Depends on existing database state
          def test_get_user():
              response = client.get("/api/users/1")  # User 1 might not exist
              assert response.json()["name"] == "John"  # Name might change
          
          # GOOD: Create test data
          def test_get_user(user_factory):
              user = user_factory(name="Test User")
              response = client.get(f"/api/users/{user.id}")
              assert response.json()["name"] == "Test User"


# =============================================================================
# SECTION 6: TEST DATA MANAGEMENT
# =============================================================================
test_data_management:
  overview: |
    Effective test data management ensures test isolation, reproducibility,
    and maintainability. Use factories for flexible data creation and
    proper cleanup strategies.

  factory_boy:
    description: "Test data factories with Factory Boy"
    code_example: |
      # factories/base.py
      import factory
      from factory.alchemy import SQLAlchemyModelFactory
      from myapp.database import Session
      from myapp.models import User, Order, Product
      
      class BaseFactory(SQLAlchemyModelFactory):
          """Base factory with session configuration."""
          
          class Meta:
              abstract = True
              sqlalchemy_session = Session
              sqlalchemy_session_persistence = "commit"
      
      # factories/user.py
      import factory
      from factory import Faker, LazyAttribute, SubFactory, LazyFunction
      from datetime import datetime, timedelta
      import secrets
      
      class UserFactory(BaseFactory):
          """Factory for creating User instances."""
          
          class Meta:
              model = User
          
          # Basic fields
          email = Faker("email")
          name = Faker("name")
          password_hash = LazyFunction(lambda: hash_password("testpassword"))
          
          # Computed fields
          username = LazyAttribute(lambda o: o.email.split("@")[0])
          
          # Date fields
          created_at = LazyFunction(datetime.utcnow)
          last_login = None
          
          # Boolean fields
          is_active = True
          is_verified = False
          
          # Traits for common variations
          class Params:
              admin = factory.Trait(
                  role="admin",
                  is_verified=True,
              )
              
              verified = factory.Trait(
                  is_verified=True,
                  verified_at=LazyFunction(datetime.utcnow),
              )
              
              inactive = factory.Trait(
                  is_active=False,
                  deactivated_at=LazyFunction(datetime.utcnow),
              )
          
          # Default role
          role = "user"
      
      # factories/product.py
      class ProductFactory(BaseFactory):
          """Factory for creating Product instances."""
          
          class Meta:
              model = Product
          
          name = Faker("catch_phrase")
          description = Faker("paragraph")
          price = Faker("pydecimal", left_digits=3, right_digits=2, positive=True)
          sku = LazyFunction(lambda: f"SKU-{secrets.token_hex(4).upper()}")
          stock_quantity = Faker("random_int", min=0, max=1000)
          is_available = True
          
          # Category relationship
          category = SubFactory("factories.CategoryFactory")
          
          class Params:
              out_of_stock = factory.Trait(
                  stock_quantity=0,
                  is_available=False,
              )
              
              on_sale = factory.Trait(
                  original_price=LazyAttribute(lambda o: o.price * 2),
                  is_on_sale=True,
              )
      
      # factories/order.py
      class OrderFactory(BaseFactory):
          """Factory for creating Order instances."""
          
          class Meta:
              model = Order
          
          user = SubFactory(UserFactory)
          status = "pending"
          total = factory.LazyAttribute(lambda o: sum(
              item.price * item.quantity for item in o.items
          ) if hasattr(o, 'items') else 0)
          
          created_at = LazyFunction(datetime.utcnow)
          
          class Params:
              completed = factory.Trait(
                  status="completed",
                  completed_at=LazyFunction(datetime.utcnow),
              )
              
              cancelled = factory.Trait(
                  status="cancelled",
                  cancelled_at=LazyFunction(datetime.utcnow),
                  cancellation_reason="Customer request",
              )
          
          @factory.post_generation
          def items(self, create, extracted, **kwargs):
              """Handle order items."""
              if not create:
                  return
              
              if extracted:
                  for item in extracted:
                      self.items.append(item)
              else:
                  # Create default items
                  OrderItemFactory.create_batch(
                      size=kwargs.get("count", 2),
                      order=self
                  )

    factory_usage:
      description: "Using factories in tests"
      code_example: |
        import pytest
        from factories import UserFactory, OrderFactory, ProductFactory
        
        class TestOrderService:
            """Tests using factories."""
            
            def test_create_order_basic(self):
                """Create order with factory defaults."""
                user = UserFactory()
                order = OrderFactory(user=user)
                
                assert order.user == user
                assert order.status == "pending"
            
            def test_create_order_with_traits(self):
                """Use traits for specific states."""
                user = UserFactory(admin=True, verified=True)
                order = OrderFactory(completed=True)
                
                assert user.role == "admin"
                assert user.is_verified
                assert order.status == "completed"
            
            def test_create_batch(self):
                """Create multiple instances."""
                users = UserFactory.create_batch(5)
                products = ProductFactory.create_batch(10, is_available=True)
                
                assert len(users) == 5
                assert all(p.is_available for p in products)
            
            def test_build_without_save(self):
                """Build instance without database save."""
                user = UserFactory.build()  # Not saved to DB
                
                assert user.id is None  # No ID yet
                assert user.email is not None
            
            def test_override_fields(self):
                """Override specific fields."""
                user = UserFactory(
                    email="specific@email.com",
                    name="Specific Name",
                    role="manager",
                )
                
                assert user.email == "specific@email.com"
                assert user.name == "Specific Name"
            
            def test_related_objects(self):
                """Create with related objects."""
                order = OrderFactory(
                    user__name="Order Owner",      # Nested attribute
                    user__email="owner@test.com",
                    items__count=5,                # Custom post-generation
                )
                
                assert order.user.name == "Order Owner"
                assert len(order.items) == 5

  test_isolation:
    description: "Strategies for test isolation"
    strategies:
      database_transactions:
        description: "Rollback transactions after each test"
        code_example: |
          import pytest
          from sqlalchemy import event
          
          @pytest.fixture
          def db_session(engine):
              """
              Database session with automatic rollback.
              Each test runs in isolation without affecting other tests.
              """
              connection = engine.connect()
              transaction = connection.begin()
              
              # Create session bound to connection
              session = Session(bind=connection)
              
              # Begin nested transaction (savepoint)
              nested = connection.begin_nested()
              
              # Restart savepoint on each commit
              @event.listens_for(session, "after_transaction_end")
              def restart_savepoint(s, trans):
                  nonlocal nested
                  if trans.nested and not trans._parent.nested:
                      nested = connection.begin_nested()
              
              yield session
              
              # Cleanup: rollback everything
              session.close()
              transaction.rollback()
              connection.close()
      
      test_database:
        description: "Separate test database"
        code_example: |
          # conftest.py
          import pytest
          import os
          
          @pytest.fixture(scope="session")
          def test_database():
              """Create isolated test database."""
              test_db_url = os.environ.get(
                  "TEST_DATABASE_URL",
                  "postgresql://test:test@localhost/test_myapp"
              )
              
              # Create test database
              create_database(test_db_url)
              
              # Run migrations
              run_migrations(test_db_url)
              
              yield test_db_url
              
              # Cleanup
              drop_database(test_db_url)
          
          @pytest.fixture
          def clean_database(db_session):
              """Clean database between tests."""
              yield db_session
              
              # Truncate all tables
              for table in reversed(Base.metadata.sorted_tables):
                  db_session.execute(table.delete())
              db_session.commit()
      
      docker_test_containers:
        description: "Ephemeral containers for integration tests"
        code_example: |
          # conftest.py
          import pytest
          import testcontainers.postgres
          import testcontainers.redis
          
          @pytest.fixture(scope="session")
          def postgres_container():
              """Spin up PostgreSQL container for tests."""
              with testcontainers.postgres.PostgresContainer(
                  "postgres:15",
                  user="test",
                  password="test",
                  dbname="test_db",
              ) as postgres:
                  yield postgres.get_connection_url()
          
          @pytest.fixture(scope="session")
          def redis_container():
              """Spin up Redis container for tests."""
              with testcontainers.redis.RedisContainer("redis:7") as redis:
                  yield redis.get_connection_url()
          
          @pytest.fixture
          def app(postgres_container, redis_container):
              """Application configured with test containers."""
              app = create_app(
                  database_url=postgres_container,
                  redis_url=redis_container,
              )
              return app

  fixtures_and_seeding:
    description: "Static test data and database seeding"
    code_example: |
      # fixtures/users.yaml
      users:
        - id: 1
          email: admin@example.com
          name: Admin User
          role: admin
          is_active: true
        
        - id: 2
          email: user@example.com
          name: Regular User
          role: user
          is_active: true
        
        - id: 3
          email: inactive@example.com
          name: Inactive User
          role: user
          is_active: false
      
      # fixtures/products.yaml
      products:
        - id: 1
          name: Basic Widget
          price: 19.99
          category: widgets
          stock: 100
        
        - id: 2
          name: Premium Widget
          price: 49.99
          category: widgets
          stock: 50
      
      # conftest.py
      import pytest
      import yaml
      from pathlib import Path
      
      @pytest.fixture
      def load_fixtures(db_session):
          """Load YAML fixtures into database."""
          def _load(fixture_name: str):
              fixture_path = Path(__file__).parent / "fixtures" / f"{fixture_name}.yaml"
              with open(fixture_path) as f:
                  data = yaml.safe_load(f)
              
              for table_name, records in data.items():
                  model = get_model_for_table(table_name)
                  for record in records:
                      instance = model(**record)
                      db_session.add(instance)
              
              db_session.commit()
          
          return _load
      
      # Usage
      def test_with_fixtures(db_session, load_fixtures):
          load_fixtures("users")
          load_fixtures("products")
          
          user = db_session.query(User).filter_by(email="admin@example.com").first()
          assert user.role == "admin"

  best_practices:
    good_examples:
      - pattern: "Minimal test data"
        description: "Create only what's needed for the test"
        example: |
          # BAD: Create everything
          def test_user_can_view_own_profile():
              # Creates unnecessary related data
              user = UserFactory(
                  orders=OrderFactory.create_batch(10),
                  reviews=ReviewFactory.create_batch(5),
                  addresses=AddressFactory.create_batch(3),
              )
              
              profile = get_user_profile(user.id)
              assert profile.name == user.name
          
          # GOOD: Minimal data
          def test_user_can_view_own_profile():
              user = UserFactory()  # Just the user, nothing else
              
              profile = get_user_profile(user.id)
              assert profile.name == user.name
      
      - pattern: "Explicit test data"
        description: "Don't rely on factory defaults for important values"
        example: |
          # BAD: Implicit dependency on factory default
          def test_admin_can_delete_users():
              admin = UserFactory()  # Is this an admin? Unclear!
              target = UserFactory()
              
              result = delete_user(admin, target.id)
              assert result.success
          
          # GOOD: Explicit role
          def test_admin_can_delete_users():
              admin = UserFactory(role="admin")  # Clearly an admin
              target = UserFactory(role="user")
              
              result = delete_user(admin, target.id)
              assert result.success

    bad_examples:
      - pattern: "Shared mutable state"
        description: "Tests modify shared data"
        example: |
          # BAD: Shared module-level data
          TEST_USER = UserFactory()  # Created once, shared by all tests
          
          def test_update_user_name():
              TEST_USER.name = "New Name"  # Modifies shared state!
              db.save(TEST_USER)
          
          def test_user_original_name():
              assert TEST_USER.name == "Original"  # Fails! State was modified
          
          # GOOD: Fresh data per test
          @pytest.fixture
          def test_user():
              return UserFactory()
          
          def test_update_user_name(test_user):
              test_user.name = "New Name"  # Only affects this test
              db.save(test_user)


# =============================================================================
# SECTION 7: TESTING CHECKLIST
# =============================================================================
testing_checklist:
  pre_development:
    - item: "Define acceptance criteria before writing code"
    - item: "Identify test pyramid distribution for feature"
    - item: "Set up test data factories if needed"
    - item: "Review existing tests for similar functionality"
  
  unit_tests:
    - item: "Test all public methods"
    - item: "Cover happy path and error cases"
    - item: "Test boundary values"
    - item: "Mock external dependencies"
    - item: "Verify exception handling"
    - item: "Test edge cases (null, empty, max values)"
  
  integration_tests:
    - item: "Test database operations (CRUD)"
    - item: "Verify transaction handling"
    - item: "Test external service integrations"
    - item: "Validate data serialization/deserialization"
    - item: "Test queue/message handling"
  
  api_tests:
    - item: "Test all HTTP methods (GET, POST, PUT, DELETE)"
    - item: "Verify response status codes"
    - item: "Validate response schemas"
    - item: "Test authentication/authorization"
    - item: "Test rate limiting"
    - item: "Test pagination"
    - item: "Test error responses"
  
  e2e_tests:
    - item: "Test critical user journeys"
    - item: "Verify cross-browser compatibility"
    - item: "Test responsive design"
    - item: "Validate form submissions"
    - item: "Test navigation flows"
    - item: "Verify loading states"
  
  performance_tests:
    - item: "Define performance baselines"
    - item: "Run load tests before release"
    - item: "Test with realistic data volumes"
    - item: "Monitor resource utilization"
    - item: "Test concurrent user scenarios"
  
  test_maintenance:
    - item: "Remove duplicate tests"
    - item: "Update tests when requirements change"
    - item: "Fix flaky tests immediately"
    - item: "Review test coverage regularly"
    - item: "Document complex test setups"
  
  ci_cd_integration:
    - item: "Run unit tests on every commit"
    - item: "Run integration tests on PR"
    - item: "Run E2E tests before deploy"
    - item: "Fail build on coverage drop"
    - item: "Generate test reports"
    - item: "Set up parallel test execution"


# =============================================================================
# SECTION 8: CI/CD TEST INTEGRATION
# =============================================================================
ci_cd_integration:
  github_actions:
    description: "Complete test pipeline with GitHub Actions"
    code_example: |
      # .github/workflows/test.yml
      name: Test Suite
      
      on:
        push:
          branches: [main, develop]
        pull_request:
          branches: [main]
      
      env:
        PYTHON_VERSION: "3.11"
        NODE_VERSION: "20"
      
      jobs:
        unit-tests:
          name: Unit Tests
          runs-on: ubuntu-latest
          steps:
            - uses: actions/checkout@v4
            
            - name: Set up Python
              uses: actions/setup-python@v5
              with:
                python-version: ${{ env.PYTHON_VERSION }}
                cache: pip
            
            - name: Install dependencies
              run: |
                pip install -r requirements-dev.txt
            
            - name: Run unit tests
              run: |
                pytest tests/unit \
                  --cov=src \
                  --cov-report=xml \
                  --cov-fail-under=80 \
                  -n auto \
                  --junitxml=junit-unit.xml
            
            - name: Upload coverage
              uses: codecov/codecov-action@v4
              with:
                files: coverage.xml
                flags: unit
        
        integration-tests:
          name: Integration Tests
          runs-on: ubuntu-latest
          services:
            postgres:
              image: postgres:15
              env:
                POSTGRES_USER: test
                POSTGRES_PASSWORD: test
                POSTGRES_DB: test_db
              ports:
                - 5432:5432
              options: >-
                --health-cmd pg_isready
                --health-interval 10s
                --health-timeout 5s
                --health-retries 5
            
            redis:
              image: redis:7
              ports:
                - 6379:6379
          
          steps:
            - uses: actions/checkout@v4
            
            - name: Set up Python
              uses: actions/setup-python@v5
              with:
                python-version: ${{ env.PYTHON_VERSION }}
            
            - name: Run integration tests
              env:
                DATABASE_URL: postgresql://test:test@localhost:5432/test_db
                REDIS_URL: redis://localhost:6379
              run: |
                pytest tests/integration \
                  --junitxml=junit-integration.xml
        
        e2e-tests:
          name: E2E Tests
          runs-on: ubuntu-latest
          steps:
            - uses: actions/checkout@v4
            
            - name: Set up Python
              uses: actions/setup-python@v5
              with:
                python-version: ${{ env.PYTHON_VERSION }}
            
            - name: Install Playwright
              run: |
                pip install playwright pytest-playwright
                playwright install --with-deps chromium
            
            - name: Start application
              run: |
                docker-compose up -d
                ./wait-for-it.sh localhost:8000 --timeout=60
            
            - name: Run E2E tests
              run: |
                pytest tests/e2e \
                  --browser chromium \
                  --screenshot on \
                  --video on
            
            - name: Upload artifacts
              if: failure()
              uses: actions/upload-artifact@v4
              with:
                name: playwright-report
                path: test-results/
        
        api-tests:
          name: API Contract Tests
          runs-on: ubuntu-latest
          steps:
            - uses: actions/checkout@v4
            
            - name: Run contract tests
              run: |
                pytest tests/contracts \
                  --junitxml=junit-contracts.xml
            
            - name: Publish Pact
              if: github.ref == 'refs/heads/main'
              run: |
                pact-broker publish pacts/ \
                  --broker-base-url=${{ secrets.PACT_BROKER_URL }} \
                  --consumer-app-version=${{ github.sha }}

  parallel_execution:
    description: "Optimize test execution with parallelization"
    code_example: |
      # pytest.ini for parallel execution
      [pytest]
      addopts = -n auto --dist loadgroup
      
      # Split tests by time (requires pytest-split)
      # pytest --splits 4 --group 1  # Run on 4 parallel jobs
      
      # GitHub Actions matrix for parallel E2E
      jobs:
        e2e-parallel:
          strategy:
            matrix:
              shard: [1, 2, 3, 4]
          steps:
            - name: Run E2E shard
              run: |
                pytest tests/e2e \
                  --splits 4 \
                  --group ${{ matrix.shard }}


# =============================================================================
# METADATA
# =============================================================================
references:
  pytest:
    - "https://docs.pytest.org/"
    - "https://pytest-cov.readthedocs.io/"
  playwright:
    - "https://playwright.dev/python/docs/intro"
  k6:
    - "https://k6.io/docs/"
  factory_boy:
    - "https://factoryboy.readthedocs.io/"
  pact:
    - "https://docs.pact.io/"

version_history:
  - version: "1.0.0"
    date: "2026-02-02"
    changes:
      - "Initial comprehensive QA/Testing mastery guide"
      - "Test pyramid strategies and ratios"
      - "Pytest patterns, fixtures, and mocking"
      - "Playwright E2E testing patterns"
      - "k6 performance testing"
      - "API testing and contract testing"
      - "Test data management with Factory Boy"
